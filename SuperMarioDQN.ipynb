{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import nes_py\n",
    "\n",
    "\n",
    "\n",
    "COMMON_PATH = \"/home/ec2-user/sample-notebooks/reinforcement_learning/common\"\n",
    "sys.path.append(COMMON_PATH)\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-eu-central-1-079978429192/\n"
     ]
    }
   ],
   "source": [
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-eu-central-1-079978429192/\n"
     ]
    }
   ],
   "source": [
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = 'rl-super-mario'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in local mode?\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = 'local'\n",
    "else:\n",
    "    instance_type = \"ml.m4.4xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using IAM role arn: arn:aws:iam::079978429192:role/service-role/AmazonSageMaker-ExecutionRole-20201119T193838\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    role = get_execution_role()\n",
    "\n",
    "print(\"Using IAM role arn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36magents\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdqn_agent\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DQNAgentParameters\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mbase_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m VisualizationParameters, PresetValidationParameters, DistributedCoachSynchronizationType\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcore_types\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36menvironments\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mgym_environment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GymEnvironmentParameters, GymVectorEnvironment\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mgraph_managers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mbasic_rl_graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BasicRLGraphManager\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mgraph_managers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mgraph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ScheduleParameters\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmemories\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmemory\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MemoryGranularity\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mschedules\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LinearSchedule\n",
      "\n",
      "\u001b[37m####################\u001b[39;49;00m\n",
      "\u001b[37m# Graph Scheduling #\u001b[39;49;00m\n",
      "\u001b[37m####################\u001b[39;49;00m\n",
      "\n",
      "schedule_params = ScheduleParameters()\n",
      "schedule_params.improve_steps = TrainingSteps(\u001b[34m10000000000\u001b[39;49;00m)\n",
      "schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(\u001b[34m10\u001b[39;49;00m)\n",
      "schedule_params.evaluation_steps = EnvironmentEpisodes(\u001b[34m1\u001b[39;49;00m)\n",
      "schedule_params.heatup_steps = EnvironmentSteps(\u001b[34m1000\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m#########\u001b[39;49;00m\n",
      "\u001b[37m# Agent #\u001b[39;49;00m\n",
      "\u001b[37m#########\u001b[39;49;00m\n",
      "agent_params = DQNAgentParameters()\n",
      "\n",
      "\u001b[37m# DQN params\u001b[39;49;00m\n",
      "agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(\u001b[34m100\u001b[39;49;00m)\n",
      "agent_params.algorithm.discount = \u001b[34m0.99\u001b[39;49;00m\n",
      "agent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# NN configuration\u001b[39;49;00m\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].learning_rate = \u001b[34m0.00025\u001b[39;49;00m\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace_mse_with_huber_loss = \u001b[34mFalse\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# ER size\u001b[39;49;00m\n",
      "agent_params.memory.max_size = (MemoryGranularity.Transitions, \u001b[34m40000\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# E-Greedy schedule\u001b[39;49;00m\n",
      "agent_params.exploration.epsilon_schedule = LinearSchedule(\u001b[34m1.0\u001b[39;49;00m, \u001b[34m0.01\u001b[39;49;00m, \u001b[34m10000\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m################\u001b[39;49;00m\n",
      "\u001b[37m#  Environment #\u001b[39;49;00m\n",
      "\u001b[37m################\u001b[39;49;00m\n",
      "env_params = GymVectorEnvironment()\n",
      "env_params.level = \u001b[33m\"\u001b[39;49;00m\u001b[33menv.gym_super_mario_bros.smb_env:SuperMarioBrosEnv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#################\u001b[39;49;00m\n",
      "\u001b[37m# Visualization #\u001b[39;49;00m\n",
      "\u001b[37m#################\u001b[39;49;00m\n",
      "\n",
      "vis_params = VisualizationParameters()\n",
      "vis_params.dump_gifs = \u001b[34mTrue\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m########\u001b[39;49;00m\n",
      "\u001b[37m# Test #\u001b[39;49;00m\n",
      "\u001b[37m########\u001b[39;49;00m\n",
      "preset_validation_params = PresetValidationParameters()\n",
      "preset_validation_params.test = \u001b[34mTrue\u001b[39;49;00m\n",
      "preset_validation_params.min_reward_threshold = \u001b[34m150\u001b[39;49;00m\n",
      "preset_validation_params.max_episodes_to_achieve_reward = \u001b[34m250\u001b[39;49;00m\n",
      "\n",
      "\n",
      "graph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params, schedule_params=schedule_params, vis_params=vis_params, preset_validation_params=preset_validation_params)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/mario-dqn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_rl\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcoach_launcher\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SageMakerCoachPresetLauncher\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMyLauncher\u001b[39;49;00m(SageMakerCoachPresetLauncher):\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mdefault_preset_name\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"This points to a .py file that configures everything about the RL job.\u001b[39;49;00m\n",
      "\u001b[33m        It can be overridden at runtime by specifying the RLCOACH_PRESET hyperparameter.\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mmario-dqn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    MyLauncher.train_main()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/train-coach.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "estimator = RLEstimator(entry_point=\"train-coach.py\",\n",
    "                        source_dir='src',\n",
    "                        toolkit=RLToolkit.COACH,\n",
    "                        toolkit_version='1.0.0',\n",
    "                        dependencies=[COMMON_PATH +\"/sagemaker_rl\"],\n",
    "                        framework=RLFramework.TENSORFLOW,\n",
    "                        role=role,\n",
    "                        train_instance_type=instance_type,\n",
    "                        train_instance_count=1,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix\n",
    "                    )\n",
    "\n",
    "estimator.fit(wait=local_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
